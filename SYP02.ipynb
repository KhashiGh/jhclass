{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f8c738e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "32798129",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.0 0.66 0.8 3.986\n",
      "1.0 0.22 0.27 1.076\n",
      "2.0 0.18 0.21 0.894\n",
      "3.0 0.14 0.17 0.791\n",
      "4.0 0.12 0.15 0.722\n",
      "5.0 0.11 0.13 0.673\n",
      "6.0 0.1 0.12 0.625\n",
      "7.0 0.09 0.11 0.588\n",
      "8.0 0.08 0.1 0.551\n",
      "9.0 0.08 0.09 0.515\n",
      "10.0 0.06 0.08 0.48\n",
      "11.0 0.06 0.08 0.447\n",
      "12.0 0.06 0.08 0.417\n",
      "13.0 0.07 0.08 0.387\n",
      "14.0 0.06 0.07 0.357\n",
      "15.0 0.06 0.07 0.325\n",
      "16.0 0.05 0.07 0.296\n",
      "17.0 0.06 0.07 0.266\n",
      "18.0 0.05 0.07 0.237\n",
      "19.0 0.05 0.07 0.208\n",
      "20.0 0.06 0.07 0.178\n",
      "21.0 0.05 0.07 0.149\n",
      "22.0 0.07 0.07 0.119\n",
      "23.0 0.04 0.07 0.09\n",
      "24.0 0.08 0.07 0.06\n",
      "25.0 0.03 0.06 0.031\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "theta = 0.55\n",
    "r_f = 0.9\n",
    "p = 0.3\n",
    "delta = 0.05\n",
    "w0 = 0.8\n",
    "c_r = 0.3\n",
    "r = 0.6\n",
    "T = 25\n",
    "t0 = 0\n",
    "\n",
    "rho = 1 / (1 + delta)\n",
    "gamma = rho\n",
    "\n",
    "best_f = 0\n",
    "w = w0\n",
    "\n",
    "# Define the transition dynamics function (for simplicity, a deterministic environment)\n",
    "def transition_function(state, action):\n",
    "        next_w = (1 - action) * state\n",
    "        next_w = np.clip(next_w, 0, 1)  # Clip to stay within [0, 1]\n",
    "        return next_w\n",
    "\n",
    "# Define the reward function (reward depends on state and action)\n",
    "def reward_function(state, action):\n",
    "    return (action * state * theta * r_f - p * action**2 + r * theta - c_r)\n",
    "\n",
    "for t in np.linspace (t0, T, T-t0+1):\n",
    "    obj = 0\n",
    "    w_next = transition_function(w, best_f)\n",
    "    for f in np.linspace(0, 1, 101):\n",
    "        cal_obj = reward_function(w, f) + gamma * (reward_function(w_next, f) * (T - t))\n",
    "        #cal_obj = ((-f * w_next * theta * r_f) + (p * (f**2)) - (r * theta) + c_r) * (rho**t)\n",
    "        if cal_obj > obj:\n",
    "            obj = round(cal_obj, 3)\n",
    "            best_f = round(f, 2)\n",
    "            w = round(w_next, 2)\n",
    "    \n",
    "    print(t, best_f, w, obj)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a23f283",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "abc73b43",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "State: 0.00, Optimal Action: 0.00, Optimal Value: 0.45\n",
      "State: 0.01, Optimal Action: 0.01, Optimal Value: 0.45\n",
      "State: 0.02, Optimal Action: 0.01, Optimal Value: 0.45\n",
      "State: 0.02, Optimal Action: 0.02, Optimal Value: 0.45\n",
      "State: 0.03, Optimal Action: 0.03, Optimal Value: 0.46\n",
      "State: 0.04, Optimal Action: 0.03, Optimal Value: 0.46\n",
      "State: 0.05, Optimal Action: 0.04, Optimal Value: 0.46\n",
      "State: 0.06, Optimal Action: 0.05, Optimal Value: 0.46\n",
      "State: 0.06, Optimal Action: 0.05, Optimal Value: 0.47\n",
      "State: 0.07, Optimal Action: 0.06, Optimal Value: 0.47\n",
      "State: 0.08, Optimal Action: 0.07, Optimal Value: 0.47\n",
      "State: 0.09, Optimal Action: 0.07, Optimal Value: 0.48\n",
      "State: 0.10, Optimal Action: 0.08, Optimal Value: 0.48\n",
      "State: 0.10, Optimal Action: 0.09, Optimal Value: 0.49\n",
      "State: 0.11, Optimal Action: 0.09, Optimal Value: 0.49\n",
      "State: 0.12, Optimal Action: 0.10, Optimal Value: 0.50\n",
      "State: 0.13, Optimal Action: 0.11, Optimal Value: 0.50\n",
      "State: 0.14, Optimal Action: 0.11, Optimal Value: 0.51\n",
      "State: 0.14, Optimal Action: 0.12, Optimal Value: 0.52\n",
      "State: 0.15, Optimal Action: 0.13, Optimal Value: 0.52\n",
      "State: 0.16, Optimal Action: 0.13, Optimal Value: 0.53\n",
      "State: 0.17, Optimal Action: 0.14, Optimal Value: 0.54\n",
      "State: 0.18, Optimal Action: 0.15, Optimal Value: 0.55\n",
      "State: 0.18, Optimal Action: 0.15, Optimal Value: 0.56\n",
      "State: 0.19, Optimal Action: 0.16, Optimal Value: 0.57\n",
      "State: 0.20, Optimal Action: 0.16, Optimal Value: 0.58\n",
      "State: 0.21, Optimal Action: 0.17, Optimal Value: 0.59\n",
      "State: 0.22, Optimal Action: 0.18, Optimal Value: 0.60\n",
      "State: 0.22, Optimal Action: 0.18, Optimal Value: 0.61\n",
      "State: 0.23, Optimal Action: 0.19, Optimal Value: 0.62\n",
      "State: 0.24, Optimal Action: 0.20, Optimal Value: 0.63\n",
      "State: 0.25, Optimal Action: 0.20, Optimal Value: 0.64\n",
      "State: 0.26, Optimal Action: 0.21, Optimal Value: 0.65\n",
      "State: 0.26, Optimal Action: 0.22, Optimal Value: 0.67\n",
      "State: 0.27, Optimal Action: 0.22, Optimal Value: 0.68\n",
      "State: 0.28, Optimal Action: 0.23, Optimal Value: 0.69\n",
      "State: 0.29, Optimal Action: 0.24, Optimal Value: 0.71\n",
      "State: 0.30, Optimal Action: 0.24, Optimal Value: 0.72\n",
      "State: 0.30, Optimal Action: 0.25, Optimal Value: 0.74\n",
      "State: 0.31, Optimal Action: 0.26, Optimal Value: 0.75\n",
      "State: 0.32, Optimal Action: 0.26, Optimal Value: 0.77\n",
      "State: 0.33, Optimal Action: 0.27, Optimal Value: 0.78\n",
      "State: 0.34, Optimal Action: 0.28, Optimal Value: 0.80\n",
      "State: 0.34, Optimal Action: 0.28, Optimal Value: 0.82\n",
      "State: 0.35, Optimal Action: 0.29, Optimal Value: 0.83\n",
      "State: 0.36, Optimal Action: 0.30, Optimal Value: 0.85\n",
      "State: 0.37, Optimal Action: 0.30, Optimal Value: 0.87\n",
      "State: 0.38, Optimal Action: 0.31, Optimal Value: 0.89\n",
      "State: 0.38, Optimal Action: 0.32, Optimal Value: 0.91\n",
      "State: 0.39, Optimal Action: 0.32, Optimal Value: 0.93\n",
      "State: 0.40, Optimal Action: 0.33, Optimal Value: 0.95\n",
      "State: 0.41, Optimal Action: 0.34, Optimal Value: 0.97\n",
      "State: 0.42, Optimal Action: 0.34, Optimal Value: 0.99\n",
      "State: 0.42, Optimal Action: 0.35, Optimal Value: 1.01\n",
      "State: 0.43, Optimal Action: 0.36, Optimal Value: 1.03\n",
      "State: 0.44, Optimal Action: 0.36, Optimal Value: 1.05\n",
      "State: 0.45, Optimal Action: 0.37, Optimal Value: 1.07\n",
      "State: 0.46, Optimal Action: 0.38, Optimal Value: 1.09\n",
      "State: 0.46, Optimal Action: 0.38, Optimal Value: 1.12\n",
      "State: 0.47, Optimal Action: 0.39, Optimal Value: 1.14\n",
      "State: 0.48, Optimal Action: 0.40, Optimal Value: 1.16\n",
      "State: 0.49, Optimal Action: 0.40, Optimal Value: 1.19\n",
      "State: 0.50, Optimal Action: 0.41, Optimal Value: 1.21\n",
      "State: 0.50, Optimal Action: 0.42, Optimal Value: 1.24\n",
      "State: 0.51, Optimal Action: 0.42, Optimal Value: 1.26\n",
      "State: 0.52, Optimal Action: 0.43, Optimal Value: 1.29\n",
      "State: 0.53, Optimal Action: 0.44, Optimal Value: 1.31\n",
      "State: 0.54, Optimal Action: 0.44, Optimal Value: 1.34\n",
      "State: 0.54, Optimal Action: 0.45, Optimal Value: 1.36\n",
      "State: 0.55, Optimal Action: 0.46, Optimal Value: 1.39\n",
      "State: 0.56, Optimal Action: 0.46, Optimal Value: 1.42\n",
      "State: 0.57, Optimal Action: 0.47, Optimal Value: 1.45\n",
      "State: 0.58, Optimal Action: 0.48, Optimal Value: 1.48\n",
      "State: 0.58, Optimal Action: 0.48, Optimal Value: 1.50\n",
      "State: 0.59, Optimal Action: 0.49, Optimal Value: 1.53\n",
      "State: 0.60, Optimal Action: 0.49, Optimal Value: 1.56\n",
      "State: 0.61, Optimal Action: 0.50, Optimal Value: 1.59\n",
      "State: 0.62, Optimal Action: 0.51, Optimal Value: 1.62\n",
      "State: 0.62, Optimal Action: 0.51, Optimal Value: 1.65\n",
      "State: 0.63, Optimal Action: 0.52, Optimal Value: 1.68\n",
      "State: 0.64, Optimal Action: 0.53, Optimal Value: 1.72\n",
      "State: 0.65, Optimal Action: 0.53, Optimal Value: 1.75\n",
      "State: 0.66, Optimal Action: 0.54, Optimal Value: 1.78\n",
      "State: 0.66, Optimal Action: 0.55, Optimal Value: 1.81\n",
      "State: 0.67, Optimal Action: 0.55, Optimal Value: 1.84\n",
      "State: 0.68, Optimal Action: 0.56, Optimal Value: 1.88\n",
      "State: 0.69, Optimal Action: 0.57, Optimal Value: 1.91\n",
      "State: 0.70, Optimal Action: 0.57, Optimal Value: 1.95\n",
      "State: 0.70, Optimal Action: 0.58, Optimal Value: 1.98\n",
      "State: 0.71, Optimal Action: 0.59, Optimal Value: 2.02\n",
      "State: 0.72, Optimal Action: 0.59, Optimal Value: 2.05\n",
      "State: 0.73, Optimal Action: 0.60, Optimal Value: 2.09\n",
      "State: 0.74, Optimal Action: 0.61, Optimal Value: 2.12\n",
      "State: 0.74, Optimal Action: 0.61, Optimal Value: 2.16\n",
      "State: 0.75, Optimal Action: 0.62, Optimal Value: 2.20\n",
      "State: 0.76, Optimal Action: 0.63, Optimal Value: 2.23\n",
      "State: 0.77, Optimal Action: 0.63, Optimal Value: 2.27\n",
      "State: 0.78, Optimal Action: 0.64, Optimal Value: 2.31\n",
      "State: 0.78, Optimal Action: 0.65, Optimal Value: 2.35\n",
      "State: 0.79, Optimal Action: 0.65, Optimal Value: 2.39\n",
      "State: 0.80, Optimal Action: 0.66, Optimal Value: 2.43\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "theta = 0.55\n",
    "r_f = 0.9\n",
    "p = 0.3\n",
    "delta = 0.05\n",
    "w0 = 0.8\n",
    "c_r = 0.3\n",
    "r = 0.6\n",
    "T = 25\n",
    "t0 = 0\n",
    "\n",
    "rho = 1 / (1 + delta)\n",
    "gamma = rho\n",
    "\n",
    "\n",
    "# Define the MDP parameters\n",
    "states = np.linspace(0, w0, 101)  # State space\n",
    "actions = np.linspace(0, 1, 101)  # Action space\n",
    "\n",
    "\n",
    "# Initialize the value function and policy\n",
    "V = np.zeros((T+1, len(states)))\n",
    "policy = np.zeros((T+1, len(states)))\n",
    "\n",
    "# Define the transition dynamics function\n",
    "def transition_function(state, action):\n",
    "    next_w = (1 - action) * state\n",
    "    return np.clip(next_w, 0, 1)  # Clip to stay within [0, 1]\n",
    "\n",
    "# Define the reward function\n",
    "def reward_function(state, action):\n",
    "    return action * state * theta * r_f - p * action**2 + r * theta - c_r\n",
    "\n",
    "# Value iteration\n",
    "for t in range(T, t0 - 1, -1):\n",
    "    for s_idx, state in enumerate(states):\n",
    "        max_value = -float(\"inf\")\n",
    "        best_action = None\n",
    "        w_next = transition_function(state, 0)  # Initialize w_next with 0 action\n",
    "        for a_idx, action in enumerate(actions):\n",
    "            w_next = transition_function(state, action)\n",
    "            immediate_reward = reward_function(state, action)\n",
    "            future_reward = gamma * V[t+1, s_idx] if t < T else 0\n",
    "            total_reward = immediate_reward + future_reward\n",
    "            if total_reward > max_value:\n",
    "                max_value = total_reward\n",
    "                best_action = action\n",
    "        V[t, s_idx] = max_value\n",
    "        policy[t, s_idx] = best_action\n",
    "\n",
    "# Extract the optimal policy and value function for t0\n",
    "best_policy = policy[t0, :]\n",
    "best_value = V[t0, :]\n",
    "\n",
    "# Print the optimal policy and value function at t0\n",
    "for s_idx, state in enumerate(states):\n",
    "    print(f\"State: {state:.2f}, Optimal Action: {best_policy[s_idx]:.2f}, Optimal Value: {best_value[s_idx]:.2f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "d24302cf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 0.8 0.66 0.161 0.161 0.161\n",
      "1 0.27 0.22 0.045 0.043 0.204\n",
      "2 0.21 0.17 0.039 0.036 0.239\n",
      "3 0.18 0.15 0.036 0.031 0.271\n",
      "4 0.15 0.13 0.035 0.028 0.299\n",
      "5 0.13 0.11 0.033 0.026 0.325\n",
      "6 0.12 0.1 0.033 0.024 0.35\n",
      "7 0.1 0.09 0.032 0.023 0.373\n",
      "8 0.09 0.07 0.032 0.022 0.394\n",
      "9 0.09 0.07 0.032 0.02 0.414\n",
      "10 0.08 0.07 0.031 0.019 0.434\n",
      "11 0.08 0.07 0.031 0.018 0.452\n",
      "12 0.07 0.06 0.031 0.017 0.469\n",
      "13 0.07 0.06 0.031 0.016 0.486\n",
      "14 0.06 0.05 0.031 0.016 0.501\n",
      "15 0.06 0.05 0.031 0.015 0.516\n",
      "16 0.06 0.05 0.031 0.014 0.53\n",
      "17 0.05 0.04 0.031 0.013 0.543\n",
      "18 0.05 0.04 0.031 0.013 0.556\n",
      "19 0.05 0.04 0.031 0.012 0.568\n",
      "20 0.05 0.04 0.03 0.011 0.58\n",
      "21 0.05 0.04 0.03 0.011 0.59\n",
      "22 0.04 0.03 0.03 0.01 0.601\n",
      "23 0.04 0.03 0.03 0.01 0.611\n",
      "24 0.04 0.03 0.03 0.009 0.62\n"
     ]
    }
   ],
   "source": [
    "w = w0\n",
    "best_f = 0\n",
    "obj = 0\n",
    "cum_obj = 0\n",
    "disc_obj = 0\n",
    "\n",
    "for t in range(0,25,1):\n",
    "    current_w = (1-best_f) * w\n",
    "    for s_idx, state in enumerate(states):\n",
    "        if round(current_w, 2) == round(state,2):\n",
    "            best_f = best_policy[s_idx]\n",
    "            w = current_w\n",
    "    obj = (best_f * w * theta * r_f - p * best_f**2 + r * theta - c_r)\n",
    "    disc_obj = rho**t * (best_f * w * theta * r_f - p * best_f**2 + r * theta - c_r)\n",
    "    cum_obj += rho**t * (best_f * w * theta * r_f - p * best_f**2 + r * theta - c_r)\n",
    "\n",
    "    print(t, round(current_w,2), best_f, round(obj,3), round(disc_obj,3), round(cum_obj,3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64b44376",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a48ceea6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "31512c7a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "State: 0.00, Optimal Action: 0.00, Optimal Value: 0.03\n",
      "State: 0.01, Optimal Action: 0.01, Optimal Value: 0.03\n",
      "State: 0.02, Optimal Action: 0.01, Optimal Value: 0.03\n",
      "State: 0.02, Optimal Action: 0.02, Optimal Value: 0.03\n",
      "State: 0.03, Optimal Action: 0.03, Optimal Value: 0.03\n",
      "State: 0.04, Optimal Action: 0.03, Optimal Value: 0.03\n",
      "State: 0.05, Optimal Action: 0.04, Optimal Value: 0.03\n",
      "State: 0.06, Optimal Action: 0.05, Optimal Value: 0.03\n",
      "State: 0.06, Optimal Action: 0.05, Optimal Value: 0.03\n",
      "State: 0.07, Optimal Action: 0.06, Optimal Value: 0.03\n",
      "State: 0.08, Optimal Action: 0.07, Optimal Value: 0.03\n",
      "State: 0.09, Optimal Action: 0.07, Optimal Value: 0.03\n",
      "State: 0.10, Optimal Action: 0.08, Optimal Value: 0.03\n",
      "State: 0.10, Optimal Action: 0.09, Optimal Value: 0.03\n",
      "State: 0.11, Optimal Action: 0.09, Optimal Value: 0.03\n",
      "State: 0.12, Optimal Action: 0.10, Optimal Value: 0.03\n",
      "State: 0.13, Optimal Action: 0.11, Optimal Value: 0.03\n",
      "State: 0.14, Optimal Action: 0.11, Optimal Value: 0.03\n",
      "State: 0.14, Optimal Action: 0.12, Optimal Value: 0.03\n",
      "State: 0.15, Optimal Action: 0.13, Optimal Value: 0.03\n",
      "State: 0.16, Optimal Action: 0.13, Optimal Value: 0.04\n",
      "State: 0.17, Optimal Action: 0.14, Optimal Value: 0.04\n",
      "State: 0.18, Optimal Action: 0.15, Optimal Value: 0.04\n",
      "State: 0.18, Optimal Action: 0.15, Optimal Value: 0.04\n",
      "State: 0.19, Optimal Action: 0.16, Optimal Value: 0.04\n",
      "State: 0.20, Optimal Action: 0.16, Optimal Value: 0.04\n",
      "State: 0.21, Optimal Action: 0.17, Optimal Value: 0.04\n",
      "State: 0.22, Optimal Action: 0.18, Optimal Value: 0.04\n",
      "State: 0.22, Optimal Action: 0.18, Optimal Value: 0.04\n",
      "State: 0.23, Optimal Action: 0.19, Optimal Value: 0.04\n",
      "State: 0.24, Optimal Action: 0.20, Optimal Value: 0.04\n",
      "State: 0.25, Optimal Action: 0.20, Optimal Value: 0.04\n",
      "State: 0.26, Optimal Action: 0.21, Optimal Value: 0.04\n",
      "State: 0.26, Optimal Action: 0.22, Optimal Value: 0.04\n",
      "State: 0.27, Optimal Action: 0.22, Optimal Value: 0.05\n",
      "State: 0.28, Optimal Action: 0.23, Optimal Value: 0.05\n",
      "State: 0.29, Optimal Action: 0.24, Optimal Value: 0.05\n",
      "State: 0.30, Optimal Action: 0.24, Optimal Value: 0.05\n",
      "State: 0.30, Optimal Action: 0.25, Optimal Value: 0.05\n",
      "State: 0.31, Optimal Action: 0.26, Optimal Value: 0.05\n",
      "State: 0.32, Optimal Action: 0.26, Optimal Value: 0.05\n",
      "State: 0.33, Optimal Action: 0.27, Optimal Value: 0.05\n",
      "State: 0.34, Optimal Action: 0.28, Optimal Value: 0.05\n",
      "State: 0.34, Optimal Action: 0.28, Optimal Value: 0.05\n",
      "State: 0.35, Optimal Action: 0.29, Optimal Value: 0.06\n",
      "State: 0.36, Optimal Action: 0.30, Optimal Value: 0.06\n",
      "State: 0.37, Optimal Action: 0.30, Optimal Value: 0.06\n",
      "State: 0.38, Optimal Action: 0.31, Optimal Value: 0.06\n",
      "State: 0.38, Optimal Action: 0.32, Optimal Value: 0.06\n",
      "State: 0.39, Optimal Action: 0.32, Optimal Value: 0.06\n",
      "State: 0.40, Optimal Action: 0.33, Optimal Value: 0.06\n",
      "State: 0.41, Optimal Action: 0.34, Optimal Value: 0.06\n",
      "State: 0.42, Optimal Action: 0.34, Optimal Value: 0.07\n",
      "State: 0.42, Optimal Action: 0.35, Optimal Value: 0.07\n",
      "State: 0.43, Optimal Action: 0.36, Optimal Value: 0.07\n",
      "State: 0.44, Optimal Action: 0.36, Optimal Value: 0.07\n",
      "State: 0.45, Optimal Action: 0.37, Optimal Value: 0.07\n",
      "State: 0.46, Optimal Action: 0.38, Optimal Value: 0.07\n",
      "State: 0.46, Optimal Action: 0.38, Optimal Value: 0.07\n",
      "State: 0.47, Optimal Action: 0.39, Optimal Value: 0.08\n",
      "State: 0.48, Optimal Action: 0.40, Optimal Value: 0.08\n",
      "State: 0.49, Optimal Action: 0.40, Optimal Value: 0.08\n",
      "State: 0.50, Optimal Action: 0.41, Optimal Value: 0.08\n",
      "State: 0.50, Optimal Action: 0.42, Optimal Value: 0.08\n",
      "State: 0.51, Optimal Action: 0.42, Optimal Value: 0.08\n",
      "State: 0.52, Optimal Action: 0.43, Optimal Value: 0.09\n",
      "State: 0.53, Optimal Action: 0.44, Optimal Value: 0.09\n",
      "State: 0.54, Optimal Action: 0.44, Optimal Value: 0.09\n",
      "State: 0.54, Optimal Action: 0.45, Optimal Value: 0.09\n",
      "State: 0.55, Optimal Action: 0.46, Optimal Value: 0.09\n",
      "State: 0.56, Optimal Action: 0.46, Optimal Value: 0.09\n",
      "State: 0.57, Optimal Action: 0.47, Optimal Value: 0.10\n",
      "State: 0.58, Optimal Action: 0.48, Optimal Value: 0.10\n",
      "State: 0.58, Optimal Action: 0.48, Optimal Value: 0.10\n",
      "State: 0.59, Optimal Action: 0.49, Optimal Value: 0.10\n",
      "State: 0.60, Optimal Action: 0.49, Optimal Value: 0.10\n",
      "State: 0.61, Optimal Action: 0.50, Optimal Value: 0.11\n",
      "State: 0.62, Optimal Action: 0.51, Optimal Value: 0.11\n",
      "State: 0.62, Optimal Action: 0.51, Optimal Value: 0.11\n",
      "State: 0.63, Optimal Action: 0.52, Optimal Value: 0.11\n",
      "State: 0.64, Optimal Action: 0.53, Optimal Value: 0.11\n",
      "State: 0.65, Optimal Action: 0.53, Optimal Value: 0.12\n",
      "State: 0.66, Optimal Action: 0.54, Optimal Value: 0.12\n",
      "State: 0.66, Optimal Action: 0.55, Optimal Value: 0.12\n",
      "State: 0.67, Optimal Action: 0.55, Optimal Value: 0.12\n",
      "State: 0.68, Optimal Action: 0.56, Optimal Value: 0.12\n",
      "State: 0.69, Optimal Action: 0.57, Optimal Value: 0.13\n",
      "State: 0.70, Optimal Action: 0.57, Optimal Value: 0.13\n",
      "State: 0.70, Optimal Action: 0.58, Optimal Value: 0.13\n",
      "State: 0.71, Optimal Action: 0.59, Optimal Value: 0.13\n",
      "State: 0.72, Optimal Action: 0.59, Optimal Value: 0.14\n",
      "State: 0.73, Optimal Action: 0.60, Optimal Value: 0.14\n",
      "State: 0.74, Optimal Action: 0.61, Optimal Value: 0.14\n",
      "State: 0.74, Optimal Action: 0.61, Optimal Value: 0.14\n",
      "State: 0.75, Optimal Action: 0.62, Optimal Value: 0.15\n",
      "State: 0.76, Optimal Action: 0.63, Optimal Value: 0.15\n",
      "State: 0.77, Optimal Action: 0.63, Optimal Value: 0.15\n",
      "State: 0.78, Optimal Action: 0.64, Optimal Value: 0.15\n",
      "State: 0.78, Optimal Action: 0.65, Optimal Value: 0.16\n",
      "State: 0.79, Optimal Action: 0.65, Optimal Value: 0.16\n",
      "State: 0.80, Optimal Action: 0.66, Optimal Value: 0.16\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "theta = 0.55\n",
    "r_f = 0.9\n",
    "p = 0.3\n",
    "delta = 0.05\n",
    "w0 = 0.8\n",
    "c_r = 0.3\n",
    "r = 0.6\n",
    "T = 25\n",
    "t0 = 0\n",
    "\n",
    "rho = 1 / (1 + delta)\n",
    "gamma = rho\n",
    "\n",
    "\n",
    "# Define the MDP parameters\n",
    "states = np.linspace(0, w0, 101)  # State space\n",
    "actions = np.linspace(0, 1, 101)  # Action space\n",
    "\n",
    "\n",
    "# Initialize the value function and policy\n",
    "V = np.zeros((max_iteration+1, len(states)))\n",
    "policy = np.zeros((max_iteration+1, len(states)))\n",
    "\n",
    "# Define the transition dynamics function\n",
    "def transition_function(state, action):\n",
    "    next_w = (1 - action) * state\n",
    "    return np.clip(next_w, 0, 1)  # Clip to stay within [0, 1]\n",
    "\n",
    "# Define the reward function\n",
    "def reward_function(state, action):\n",
    "    return action * state * theta * r_f - p * action**2 + r * theta - c_r\n",
    "\n",
    "max_iteration = 100\n",
    "\n",
    "# Value iteration\n",
    "for t in range(max_iteration):\n",
    "    for s_idx, state in enumerate(states):\n",
    "        max_value = -float(\"inf\")\n",
    "        best_action = None\n",
    "        w_next = transition_function(state, 0)  # Initialize w_next with 0 action\n",
    "        for a_idx, action in enumerate(actions):\n",
    "            w_next = transition_function(state, action)\n",
    "            immediate_reward = reward_function(state, action)\n",
    "            future_reward = gamma * V[t+1, s_idx] if t < max_iteration else 0\n",
    "            total_reward = immediate_reward + future_reward\n",
    "            if total_reward > max_value:\n",
    "                max_value = total_reward\n",
    "                best_action = action\n",
    "        V[t, s_idx] = max_value\n",
    "        policy[t, s_idx] = best_action\n",
    "        \n",
    "# Extract the optimal policy and value function for t0\n",
    "best_policy = policy[t0, :]\n",
    "best_value = V[t0, :]\n",
    "\n",
    "# Print the optimal policy and value function at t0\n",
    "for s_idx, state in enumerate(states):\n",
    "    print(f\"State: {state:.2f}, Optimal Action: {best_policy[s_idx]:.2f}, Optimal Value: {best_value[s_idx]:.2f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "b10f0fb0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 0.8 0.66 0.161 0.161 0.161\n",
      "1 0.27 0.22 0.045 0.043 0.204\n",
      "2 0.21 0.17 0.039 0.036 0.239\n",
      "3 0.18 0.15 0.036 0.031 0.271\n",
      "4 0.15 0.13 0.035 0.028 0.299\n",
      "5 0.13 0.11 0.033 0.026 0.325\n",
      "6 0.12 0.1 0.033 0.024 0.35\n",
      "7 0.1 0.09 0.032 0.023 0.373\n",
      "8 0.09 0.07 0.032 0.022 0.394\n",
      "9 0.09 0.07 0.032 0.02 0.414\n",
      "10 0.08 0.07 0.031 0.019 0.434\n",
      "11 0.08 0.07 0.031 0.018 0.452\n",
      "12 0.07 0.06 0.031 0.017 0.469\n",
      "13 0.07 0.06 0.031 0.016 0.486\n",
      "14 0.06 0.05 0.031 0.016 0.501\n",
      "15 0.06 0.05 0.031 0.015 0.516\n",
      "16 0.06 0.05 0.031 0.014 0.53\n",
      "17 0.05 0.04 0.031 0.013 0.543\n",
      "18 0.05 0.04 0.031 0.013 0.556\n",
      "19 0.05 0.04 0.031 0.012 0.568\n",
      "20 0.05 0.04 0.03 0.011 0.58\n",
      "21 0.05 0.04 0.03 0.011 0.59\n",
      "22 0.04 0.03 0.03 0.01 0.601\n",
      "23 0.04 0.03 0.03 0.01 0.611\n",
      "24 0.04 0.03 0.03 0.009 0.62\n"
     ]
    }
   ],
   "source": [
    "w = w0\n",
    "best_f = 0\n",
    "obj = 0\n",
    "cum_obj = 0\n",
    "disc_obj = 0\n",
    "\n",
    "for t in range(0,25,1):\n",
    "    current_w = (1-best_f) * w\n",
    "    for s_idx, state in enumerate(states):\n",
    "        if round(current_w, 2) == round(state,2):\n",
    "            best_f = best_policy[s_idx]\n",
    "            w = current_w\n",
    "    obj = (best_f * w * theta * r_f - p * best_f**2 + r * theta - c_r)\n",
    "    disc_obj = rho**t * (best_f * w * theta * r_f - p * best_f**2 + r * theta - c_r)\n",
    "    cum_obj += rho**t * (best_f * w * theta * r_f - p * best_f**2 + r * theta - c_r)\n",
    "\n",
    "    print(t, round(current_w,2), best_f, round(obj,3), round(disc_obj,3), round(cum_obj,3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db1ebe84",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "f00b86ef",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\f",
      "\n"
     ]
    }
   ],
   "source": [
    "clear all"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "6284d4eb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "State: 0.00, Optimal Action: 0.00, Optimal Value: 0.03\n",
      "State: 0.01, Optimal Action: 0.01, Optimal Value: 0.03\n",
      "State: 0.02, Optimal Action: 0.01, Optimal Value: 0.03\n",
      "State: 0.02, Optimal Action: 0.02, Optimal Value: 0.03\n",
      "State: 0.03, Optimal Action: 0.03, Optimal Value: 0.03\n",
      "State: 0.04, Optimal Action: 0.03, Optimal Value: 0.03\n",
      "State: 0.05, Optimal Action: 0.04, Optimal Value: 0.03\n",
      "State: 0.06, Optimal Action: 0.05, Optimal Value: 0.03\n",
      "State: 0.06, Optimal Action: 0.05, Optimal Value: 0.03\n",
      "State: 0.07, Optimal Action: 0.06, Optimal Value: 0.03\n",
      "State: 0.08, Optimal Action: 0.07, Optimal Value: 0.03\n",
      "State: 0.09, Optimal Action: 0.07, Optimal Value: 0.03\n",
      "State: 0.10, Optimal Action: 0.08, Optimal Value: 0.03\n",
      "State: 0.10, Optimal Action: 0.09, Optimal Value: 0.03\n",
      "State: 0.11, Optimal Action: 0.09, Optimal Value: 0.03\n",
      "State: 0.12, Optimal Action: 0.10, Optimal Value: 0.03\n",
      "State: 0.13, Optimal Action: 0.11, Optimal Value: 0.03\n",
      "State: 0.14, Optimal Action: 0.11, Optimal Value: 0.03\n",
      "State: 0.14, Optimal Action: 0.12, Optimal Value: 0.03\n",
      "State: 0.15, Optimal Action: 0.13, Optimal Value: 0.03\n",
      "State: 0.16, Optimal Action: 0.13, Optimal Value: 0.04\n",
      "State: 0.17, Optimal Action: 0.14, Optimal Value: 0.04\n",
      "State: 0.18, Optimal Action: 0.15, Optimal Value: 0.04\n",
      "State: 0.18, Optimal Action: 0.15, Optimal Value: 0.04\n",
      "State: 0.19, Optimal Action: 0.16, Optimal Value: 0.04\n",
      "State: 0.20, Optimal Action: 0.16, Optimal Value: 0.04\n",
      "State: 0.21, Optimal Action: 0.17, Optimal Value: 0.04\n",
      "State: 0.22, Optimal Action: 0.18, Optimal Value: 0.04\n",
      "State: 0.22, Optimal Action: 0.18, Optimal Value: 0.04\n",
      "State: 0.23, Optimal Action: 0.19, Optimal Value: 0.04\n",
      "State: 0.24, Optimal Action: 0.20, Optimal Value: 0.04\n",
      "State: 0.25, Optimal Action: 0.20, Optimal Value: 0.04\n",
      "State: 0.26, Optimal Action: 0.21, Optimal Value: 0.04\n",
      "State: 0.26, Optimal Action: 0.22, Optimal Value: 0.04\n",
      "State: 0.27, Optimal Action: 0.22, Optimal Value: 0.05\n",
      "State: 0.28, Optimal Action: 0.23, Optimal Value: 0.05\n",
      "State: 0.29, Optimal Action: 0.24, Optimal Value: 0.05\n",
      "State: 0.30, Optimal Action: 0.24, Optimal Value: 0.05\n",
      "State: 0.30, Optimal Action: 0.25, Optimal Value: 0.05\n",
      "State: 0.31, Optimal Action: 0.26, Optimal Value: 0.05\n",
      "State: 0.32, Optimal Action: 0.26, Optimal Value: 0.05\n",
      "State: 0.33, Optimal Action: 0.27, Optimal Value: 0.05\n",
      "State: 0.34, Optimal Action: 0.28, Optimal Value: 0.05\n",
      "State: 0.34, Optimal Action: 0.28, Optimal Value: 0.05\n",
      "State: 0.35, Optimal Action: 0.29, Optimal Value: 0.06\n",
      "State: 0.36, Optimal Action: 0.30, Optimal Value: 0.06\n",
      "State: 0.37, Optimal Action: 0.30, Optimal Value: 0.06\n",
      "State: 0.38, Optimal Action: 0.31, Optimal Value: 0.06\n",
      "State: 0.38, Optimal Action: 0.32, Optimal Value: 0.06\n",
      "State: 0.39, Optimal Action: 0.32, Optimal Value: 0.06\n",
      "State: 0.40, Optimal Action: 0.33, Optimal Value: 0.06\n",
      "State: 0.41, Optimal Action: 0.34, Optimal Value: 0.06\n",
      "State: 0.42, Optimal Action: 0.34, Optimal Value: 0.07\n",
      "State: 0.42, Optimal Action: 0.35, Optimal Value: 0.07\n",
      "State: 0.43, Optimal Action: 0.36, Optimal Value: 0.07\n",
      "State: 0.44, Optimal Action: 0.36, Optimal Value: 0.07\n",
      "State: 0.45, Optimal Action: 0.37, Optimal Value: 0.07\n",
      "State: 0.46, Optimal Action: 0.38, Optimal Value: 0.07\n",
      "State: 0.46, Optimal Action: 0.38, Optimal Value: 0.07\n",
      "State: 0.47, Optimal Action: 0.39, Optimal Value: 0.08\n",
      "State: 0.48, Optimal Action: 0.40, Optimal Value: 0.08\n",
      "State: 0.49, Optimal Action: 0.40, Optimal Value: 0.08\n",
      "State: 0.50, Optimal Action: 0.41, Optimal Value: 0.08\n",
      "State: 0.50, Optimal Action: 0.42, Optimal Value: 0.08\n",
      "State: 0.51, Optimal Action: 0.42, Optimal Value: 0.08\n",
      "State: 0.52, Optimal Action: 0.43, Optimal Value: 0.09\n",
      "State: 0.53, Optimal Action: 0.44, Optimal Value: 0.09\n",
      "State: 0.54, Optimal Action: 0.44, Optimal Value: 0.09\n",
      "State: 0.54, Optimal Action: 0.45, Optimal Value: 0.09\n",
      "State: 0.55, Optimal Action: 0.46, Optimal Value: 0.09\n",
      "State: 0.56, Optimal Action: 0.46, Optimal Value: 0.09\n",
      "State: 0.57, Optimal Action: 0.47, Optimal Value: 0.10\n",
      "State: 0.58, Optimal Action: 0.48, Optimal Value: 0.10\n",
      "State: 0.58, Optimal Action: 0.48, Optimal Value: 0.10\n",
      "State: 0.59, Optimal Action: 0.49, Optimal Value: 0.10\n",
      "State: 0.60, Optimal Action: 0.49, Optimal Value: 0.10\n",
      "State: 0.61, Optimal Action: 0.50, Optimal Value: 0.11\n",
      "State: 0.62, Optimal Action: 0.51, Optimal Value: 0.11\n",
      "State: 0.62, Optimal Action: 0.51, Optimal Value: 0.11\n",
      "State: 0.63, Optimal Action: 0.52, Optimal Value: 0.11\n",
      "State: 0.64, Optimal Action: 0.53, Optimal Value: 0.11\n",
      "State: 0.65, Optimal Action: 0.53, Optimal Value: 0.12\n",
      "State: 0.66, Optimal Action: 0.54, Optimal Value: 0.12\n",
      "State: 0.66, Optimal Action: 0.55, Optimal Value: 0.12\n",
      "State: 0.67, Optimal Action: 0.55, Optimal Value: 0.12\n",
      "State: 0.68, Optimal Action: 0.56, Optimal Value: 0.12\n",
      "State: 0.69, Optimal Action: 0.57, Optimal Value: 0.13\n",
      "State: 0.70, Optimal Action: 0.57, Optimal Value: 0.13\n",
      "State: 0.70, Optimal Action: 0.58, Optimal Value: 0.13\n",
      "State: 0.71, Optimal Action: 0.59, Optimal Value: 0.13\n",
      "State: 0.72, Optimal Action: 0.59, Optimal Value: 0.14\n",
      "State: 0.73, Optimal Action: 0.60, Optimal Value: 0.14\n",
      "State: 0.74, Optimal Action: 0.61, Optimal Value: 0.14\n",
      "State: 0.74, Optimal Action: 0.61, Optimal Value: 0.14\n",
      "State: 0.75, Optimal Action: 0.62, Optimal Value: 0.15\n",
      "State: 0.76, Optimal Action: 0.63, Optimal Value: 0.15\n",
      "State: 0.77, Optimal Action: 0.63, Optimal Value: 0.15\n",
      "State: 0.78, Optimal Action: 0.64, Optimal Value: 0.15\n",
      "State: 0.78, Optimal Action: 0.65, Optimal Value: 0.16\n",
      "State: 0.79, Optimal Action: 0.65, Optimal Value: 0.16\n",
      "State: 0.80, Optimal Action: 0.66, Optimal Value: 0.16\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "theta = 0.55\n",
    "r_f = 0.9\n",
    "p = 0.3\n",
    "delta = 0.05\n",
    "w0 = 0.8\n",
    "c_r = 0.3\n",
    "r = 0.6\n",
    "T = 25\n",
    "t0 = 0\n",
    "\n",
    "rho = 1 / (1 + delta)\n",
    "gamma = rho\n",
    "\n",
    "\n",
    "# Define the MDP parameters\n",
    "states = np.linspace(0, w0, 101)  # State space\n",
    "actions = np.linspace(0, 1, 101)  # Action space\n",
    "\n",
    "max_iteration = 5\n",
    "\n",
    "# Initialize the value function and policy\n",
    "V = np.zeros((max_iteration+1, len(states)))\n",
    "policy = np.zeros((max_iteration+1, len(states)))\n",
    "\n",
    "# Define the transition dynamics function\n",
    "def transition_function(state, action):\n",
    "    next_w = (1 - action) * state\n",
    "    return np.clip(next_w, 0, 1)  # Clip to stay within [0, 1]\n",
    "\n",
    "# Define the reward function\n",
    "def reward_function(state, action):\n",
    "    return action * state * theta * r_f - p * action**2 + r * theta - c_r\n",
    "\n",
    "# Value iteration\n",
    "for t in range(max_iteration):\n",
    "    for s_idx, state in enumerate(states):\n",
    "        max_value = -float(\"inf\")\n",
    "        best_action = None\n",
    "        w_next = transition_function(state, 0)  # Initialize w_next with 0 action\n",
    "        for a_idx, action in enumerate(actions):\n",
    "            w_next = transition_function(state, action)\n",
    "            immediate_reward = reward_function(state, action)\n",
    "            future_reward = gamma * V[t+1, s_idx] if t < max_iteration else 0\n",
    "            total_reward = immediate_reward + future_reward\n",
    "            if total_reward > max_value:\n",
    "                max_value = total_reward\n",
    "                best_action = action\n",
    "        V[t, s_idx] = max_value\n",
    "        policy[t, s_idx] = best_action\n",
    "        \n",
    "\n",
    "# Extract the optimal policy and value function for t0\n",
    "best_policy = policy[t0, :]\n",
    "best_value = V[t0, :]\n",
    "\n",
    "# Print the optimal policy and value function at t0\n",
    "for s_idx, state in enumerate(states):\n",
    "    print(f\"State: {state:.2f}, Optimal Action: {best_policy[s_idx]:.2f}, Optimal Value: {best_value[s_idx]:.2f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91b43122",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8cb1eee",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Finitie with terminal condition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "63153bdf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 0.0 0.31 32.964800455935865\n",
      "1 0.0 0.31 32.964800455935865\n",
      "2 0.0 0.31 32.964800455935865\n",
      "3 0.0 0.31 32.964800455935865\n",
      "4 0.0 0.31 32.964800455935865\n",
      "5 0.0 0.31 32.964800455935865\n",
      "6 0.0 0.31 32.964800455935865\n",
      "7 0.0 0.31 32.964800455935865\n",
      "8 0.0 0.31 32.964800455935865\n",
      "9 0.0 0.31 32.964800455935865\n",
      "10 0.0 0.31 32.964800455935865\n",
      "11 0.0 0.31 32.964800455935865\n",
      "12 0.0 0.31 32.964800455935865\n",
      "13 0.0 0.31 32.964800455935865\n",
      "14 0.0 0.31 32.964800455935865\n",
      "15 0.0 0.31 32.964800455935865\n",
      "16 0.0 0.31 32.964800455935865\n",
      "17 0.0 0.31 32.964800455935865\n",
      "18 0.0 0.31 32.964800455935865\n",
      "19 0.0 0.31 32.964800455935865\n",
      "20 0.0 0.31 32.964800455935865\n",
      "21 0.0 0.31 32.964800455935865\n",
      "22 0.0 0.31 32.964800455935865\n",
      "23 0.0 0.31 32.964800455935865\n",
      "24 0.0 0.31 32.964800455935865\n",
      "25 0.0 0.31 32.964800455935865\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "theta = 0.55\n",
    "r_f = 0.9\n",
    "p = 0.3\n",
    "delta = 0.05\n",
    "w0 = 0.8\n",
    "c_r = 0.3\n",
    "r = 0.6\n",
    "T = 25\n",
    "t0 = 0\n",
    "\n",
    "rho = 1 / (1 + delta)\n",
    "gamma = rho\n",
    "\n",
    "# Define the transition dynamics function\n",
    "def transition_function(state, action):\n",
    "    next_w = (1 - action) * state\n",
    "    return np.clip(next_w, 0, 1)  # Clip to stay within [0, 1]\n",
    "\n",
    "# Define the reward function\n",
    "def reward_function(state, action):\n",
    "    return action * state * theta * r_f - p * action**2 + r * theta - c_r\n",
    "\n",
    "npv = 0\n",
    "cal_npv = 0\n",
    "\n",
    "for t in range(0,26,1):\n",
    "    for w in np.linspace(0.1, 0.8, 81):\n",
    "        for f in np.linspace(0, 1, 101):\n",
    "            for s in range(t,26,1):\n",
    "                w_next = transition_function(w,f)\n",
    "                cal_npv += reward_function(w,f) + gamma * reward_function(w_next, f)\n",
    "                w = w_next\n",
    "                if cal_npv > npv:\n",
    "                    npv = cal_npv\n",
    "                    best_f = f\n",
    "    print(t, w, best_f, npv)\n",
    "                \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "354de705",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42401038",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea9141cb",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cbf5ec6b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Infinite time horizon"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "eae5d6b1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration: 0 Best f: 0.66 Objective Value: 0.3137085714285715\n",
      "Iteration: 1 Best f: 0.45 Objective Value: 0.17586714285714292\n",
      "Iteration: 2 Best f: 0.18 Objective Value: 0.07652411428571435\n",
      "Iteration: 3 Best f: 0.11 Objective Value: 0.06599142514285722\n",
      "Iteration: 4 Best f: 0.1 Objective Value: 0.06393351634285717\n",
      "Iteration: 5 Best f: 0.09 Objective Value: 0.06286006929028574\n",
      "Iteration: 6 Best f: 0.08 Objective Value: 0.06208625644909716\n",
      "Iteration: 7 Best f: 0.07 Objective Value: 0.06151441782870378\n",
      "Iteration: 8 Best f: 0.07 Objective Value: 0.06107652564302887\n",
      "Iteration: 9 Best f: 0.06 Objective Value: 0.06074757737993282\n",
      "Iteration: 10 Best f: 0.06 Objective Value: 0.060467772164451095\n",
      "Iteration: 11 Best f: 0.05 Objective Value: 0.060244326290724765\n",
      "Iteration: 12 Best f: 0.05 Objective Value: 0.060070914141129764\n",
      "Iteration: 13 Best f: 0.05 Objective Value: 0.05992272557693044\n",
      "Iteration: 14 Best f: 0.05 Objective Value: 0.059781946440941036\n",
      "Iteration: 15 Best f: 0.04 Objective Value: 0.05966713643797239\n",
      "Iteration: 16 Best f: 0.04 Objective Value: 0.05957514994272558\n",
      "Iteration: 17 Best f: 0.04 Objective Value: 0.059497515373587995\n",
      "Iteration: 18 Best f: 0.04 Objective Value: 0.05942298618721588\n",
      "Iteration: 19 Best f: 0.04 Objective Value: 0.05935143816829866\n",
      "Iteration: 20 Best f: 0.04 Objective Value: 0.05928275207013815\n",
      "Iteration: 21 Best f: 0.03 Objective Value: 0.05923118149049946\n",
      "Iteration: 22 Best f: 0.03 Objective Value: 0.059189374415397025\n",
      "Iteration: 23 Best f: 0.03 Objective Value: 0.059155021754363726\n",
      "Iteration: 24 Best f: 0.03 Objective Value: 0.05912169967316139\n",
      "Iteration: 25 Best f: 0.03 Objective Value: 0.05908937725439509\n",
      "Iteration: 26 Best f: 0.03 Objective Value: 0.059058024508191795\n",
      "Iteration: 27 Best f: 0.03 Objective Value: 0.05902761234437463\n",
      "Iteration: 28 Best f: 0.03 Objective Value: 0.05899811254547195\n",
      "Iteration: 29 Best f: 0.03 Objective Value: 0.058969497740536336\n",
      "Iteration: 30 Best f: 0.03 Objective Value: 0.058941741379748815\n",
      "Iteration: 31 Best f: 0.02 Objective Value: 0.058917497520809076\n",
      "Iteration: 32 Best f: 0.02 Objective Value: 0.058902873732501825\n",
      "Iteration: 33 Best f: 0.02 Objective Value: 0.058891559114994635\n",
      "Iteration: 34 Best f: 0.02 Objective Value: 0.058880470789837616\n",
      "Iteration: 35 Best f: 0.02 Objective Value: 0.05886960423118369\n",
      "Iteration: 36 Best f: 0.02 Objective Value: 0.058858955003702876\n",
      "Iteration: 37 Best f: 0.02 Objective Value: 0.0588485187607717\n",
      "Iteration: 38 Best f: 0.02 Objective Value: 0.0588382912426991\n",
      "Iteration: 39 Best f: 0.02 Objective Value: 0.058828268274987994\n",
      "Iteration: 40 Best f: 0.02 Objective Value: 0.05881844576663112\n",
      "Iteration: 41 Best f: 0.02 Objective Value: 0.05880881970844133\n",
      "Iteration: 42 Best f: 0.02 Objective Value: 0.05879938617141538\n",
      "Iteration: 43 Best f: 0.02 Objective Value: 0.05879014130512993\n",
      "Iteration: 44 Best f: 0.02 Objective Value: 0.058781081336170146\n",
      "Iteration: 45 Best f: 0.02 Objective Value: 0.05877220256658959\n",
      "Iteration: 46 Best f: 0.02 Objective Value: 0.058763501372400676\n",
      "Iteration: 47 Best f: 0.02 Objective Value: 0.05875497420209556\n",
      "Iteration: 48 Best f: 0.02 Objective Value: 0.05874661757519651\n",
      "Iteration: 49 Best f: 0.02 Objective Value: 0.05873842808083543\n",
      "Iteration: 50 Best f: 0.02 Objective Value: 0.058730402376361596\n",
      "Iteration: 51 Best f: 0.02 Objective Value: 0.058722537185977225\n",
      "Iteration: 52 Best f: 0.02 Objective Value: 0.05871482929940054\n",
      "Iteration: 53 Best f: 0.02 Objective Value: 0.058707275570555384\n",
      "Iteration: 54 Best f: 0.02 Objective Value: 0.05869987291628713\n",
      "Iteration: 55 Best f: 0.02 Objective Value: 0.058692618315104236\n",
      "Iteration: 56 Best f: 0.01 Objective Value: 0.05868704011725821\n",
      "Iteration: 57 Best f: 0.01 Objective Value: 0.05868439734109489\n",
      "Iteration: 58 Best f: 0.01 Objective Value: 0.05868268193911253\n",
      "Iteration: 59 Best f: 0.01 Objective Value: 0.058680983691149956\n",
      "Iteration: 60 Best f: 0.01 Objective Value: 0.05867930242566703\n",
      "Iteration: 61 Best f: 0.01 Objective Value: 0.05867763797283897\n",
      "Iteration: 62 Best f: 0.01 Objective Value: 0.058675990164539185\n",
      "Iteration: 63 Best f: 0.01 Objective Value: 0.05867435883432234\n",
      "Iteration: 64 Best f: 0.01 Objective Value: 0.058672743817407655\n",
      "Iteration: 65 Best f: 0.01 Objective Value: 0.05867114495066215\n",
      "Iteration: 66 Best f: 0.01 Objective Value: 0.0586695620725841\n",
      "Iteration: 67 Best f: 0.01 Objective Value: 0.058667995023286845\n",
      "Iteration: 68 Best f: 0.01 Objective Value: 0.05866644364448254\n",
      "Iteration: 69 Best f: 0.01 Objective Value: 0.05866490777946626\n",
      "Iteration: 70 Best f: 0.01 Objective Value: 0.05866338727310019\n",
      "Iteration: 71 Best f: 0.01 Objective Value: 0.058661881971797766\n",
      "Iteration: 72 Best f: 0.01 Objective Value: 0.05866039172350834\n",
      "Iteration: 73 Best f: 0.01 Objective Value: 0.05865891637770182\n",
      "Iteration: 74 Best f: 0.01 Objective Value: 0.05865745578535335\n",
      "Iteration: 75 Best f: 0.01 Objective Value: 0.05865600979892842\n",
      "Iteration: 76 Best f: 0.01 Objective Value: 0.058654578272367724\n",
      "Iteration: 77 Best f: 0.01 Objective Value: 0.0586531610610726\n",
      "Iteration: 78 Best f: 0.01 Objective Value: 0.05865175802189045\n",
      "Iteration: 79 Best f: 0.01 Objective Value: 0.05865036901310011\n",
      "Iteration: 80 Best f: 0.01 Objective Value: 0.05864899389439768\n",
      "Iteration: 81 Best f: 0.01 Objective Value: 0.05864763252688229\n",
      "Iteration: 82 Best f: 0.01 Objective Value: 0.05864628477304204\n",
      "Iteration: 83 Best f: 0.01 Objective Value: 0.058644950496740206\n",
      "Iteration: 84 Best f: 0.01 Objective Value: 0.05864362956320141\n",
      "Iteration: 85 Best f: 0.01 Objective Value: 0.05864232183899794\n",
      "Iteration: 86 Best f: 0.01 Objective Value: 0.05864102719203648\n",
      "Iteration: 87 Best f: 0.01 Objective Value: 0.0586397454915447\n",
      "Iteration: 88 Best f: 0.01 Objective Value: 0.05863847660805785\n",
      "Iteration: 89 Best f: 0.01 Objective Value: 0.058637220413405855\n",
      "Iteration: 90 Best f: 0.01 Objective Value: 0.05863597678070036\n",
      "Iteration: 91 Best f: 0.01 Objective Value: 0.05863474558432191\n",
      "Iteration: 92 Best f: 0.01 Objective Value: 0.05863352669990725\n",
      "Iteration: 93 Best f: 0.01 Objective Value: 0.05863232000433678\n",
      "Iteration: 94 Best f: 0.01 Objective Value: 0.058631125375722\n",
      "Iteration: 95 Best f: 0.01 Objective Value: 0.05862994269339336\n",
      "Iteration: 96 Best f: 0.01 Objective Value: 0.05862877183788797\n",
      "Iteration: 97 Best f: 0.01 Objective Value: 0.05862761269093763\n",
      "Iteration: 98 Best f: 0.01 Objective Value: 0.05862646513545687\n",
      "Iteration: 99 Best f: 0.01 Objective Value: 0.058625329055530875\n",
      "Iteration: 100 Best f: 0.01 Objective Value: 0.0586242043364041\n",
      "Iteration: 101 Best f: 0.01 Objective Value: 0.058623090864468624\n",
      "Iteration: 102 Best f: 0.01 Objective Value: 0.05862198852725255\n",
      "Iteration: 103 Best f: 0.01 Objective Value: 0.058620897213408615\n",
      "Iteration: 104 Best f: 0.01 Objective Value: 0.058619816812703104\n",
      "Iteration: 105 Best f: 0.01 Objective Value: 0.05861874721600464\n",
      "Iteration: 106 Best f: 0.01 Objective Value: 0.05861768831527313\n",
      "Iteration: 107 Best f: 0.01 Objective Value: 0.058616640003549\n",
      "Iteration: 108 Best f: 0.01 Objective Value: 0.05861560217494207\n",
      "Iteration: 109 Best f: 0.01 Objective Value: 0.058614574724621206\n",
      "Iteration: 110 Best f: 0.01 Objective Value: 0.05861355754880359\n",
      "Iteration: 111 Best f: 0.01 Objective Value: 0.05861255054474409\n",
      "Iteration: 112 Best f: 0.01 Objective Value: 0.058611553610725196\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "theta = 0.55\n",
    "r_f = 0.9\n",
    "p = 0.3\n",
    "delta = 0.05\n",
    "w0 = 0.8\n",
    "c_r = 0.3\n",
    "r = 0.6\n",
    "t0 = 0\n",
    "\n",
    "rho = 1 / (1 + delta)\n",
    "gamma = rho\n",
    "\n",
    "best_f = 0\n",
    "w = w0\n",
    "\n",
    "# Define the transition dynamics function (for simplicity, a deterministic environment)\n",
    "def transition_function(state, action):\n",
    "    next_w = (1 - action) * state\n",
    "    next_w = np.clip(next_w, 0, 1)  # Clip to stay within [0, 1]\n",
    "    return next_w\n",
    "\n",
    "# Define the reward function (reward depends on state and action)\n",
    "def reward_function(state, action):\n",
    "    return (action * state * theta * r_f - p * action**2 + r * theta - c_r)\n",
    "\n",
    "convergence_threshold = 1e-6  # Set your desired convergence threshold\n",
    "converged = False\n",
    "iteration = 0\n",
    "\n",
    "while not converged:\n",
    "    obj = 0\n",
    "    w_next = transition_function(w, best_f)\n",
    "    for f in np.linspace(0, 1, 101):\n",
    "        cal_obj = reward_function(w, f) + gamma * reward_function(w_next, f)\n",
    "        if cal_obj > obj:\n",
    "            obj = cal_obj\n",
    "            best_f = f\n",
    "\n",
    "    # Check for convergence by comparing the change in the objective value\n",
    "    if iteration > 0 and abs(obj - prev_obj) < convergence_threshold:\n",
    "        converged = True\n",
    "\n",
    "    prev_obj = obj\n",
    "    w = w_next\n",
    "\n",
    "    print(\"Iteration:\", iteration, \"Best f:\", best_f, \"Objective Value:\", obj)\n",
    "    iteration += 1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f534e97b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Backward induction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "8af845e8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.0 0.03\n",
      "0.01 0.03\n",
      "0.02 0.03\n",
      "0.03 0.03\n",
      "0.04 0.03\n",
      "0.05 0.029\n",
      "0.06 0.029\n",
      "0.07 0.029\n",
      "0.08 0.028\n",
      "0.09 0.028\n",
      "0.1 0.027\n",
      "0.11 0.026\n",
      "0.12 0.026\n",
      "0.13 0.025\n",
      "0.14 0.024\n",
      "0.15 0.023\n",
      "0.16 0.022\n",
      "0.17 0.021\n",
      "0.18 0.02\n",
      "0.19 0.019\n",
      "0.2 0.018\n",
      "0.21 0.017\n",
      "0.22 0.015\n",
      "0.23 0.014\n",
      "0.24 0.013\n",
      "0.25 0.011\n",
      "0.26 0.01\n",
      "0.27 0.008\n",
      "0.28 0.006\n",
      "0.29 0.005\n",
      "0.3 0.003\n",
      "0.31 0.001\n",
      "0.32 -0.001\n",
      "0.33 -0.003\n",
      "0.34 -0.005\n",
      "0.35 -0.007\n",
      "0.36 -0.009\n",
      "0.37 -0.011\n",
      "0.38 -0.013\n",
      "0.39 -0.016\n",
      "0.4 -0.018\n",
      "0.41 -0.02\n",
      "0.42 -0.023\n",
      "0.43 -0.025\n",
      "0.44 -0.028\n",
      "0.45 -0.031\n",
      "0.46 -0.033\n",
      "0.47 -0.036\n",
      "0.48 -0.039\n",
      "0.49 -0.042\n",
      "0.5 -0.045\n",
      "0.51 -0.048\n",
      "0.52 -0.051\n",
      "0.53 -0.054\n",
      "0.54 -0.057\n",
      "0.55 -0.061\n",
      "0.56 -0.064\n",
      "0.57 -0.067\n",
      "0.58 -0.071\n",
      "0.59 -0.074\n",
      "0.6 -0.078\n",
      "0.61 -0.082\n",
      "0.62 -0.085\n",
      "0.63 -0.089\n",
      "0.64 -0.093\n",
      "0.65 -0.097\n",
      "0.66 -0.101\n",
      "0.67 -0.105\n",
      "0.68 -0.109\n",
      "0.69 -0.113\n",
      "0.7 -0.117\n",
      "0.71 -0.121\n",
      "0.72 -0.126\n",
      "0.73 -0.13\n",
      "0.74 -0.134\n",
      "0.75 -0.139\n",
      "0.76 -0.143\n",
      "0.77 -0.148\n",
      "0.78 -0.153\n",
      "0.79 -0.157\n",
      "0.8 -0.162\n",
      "0.81 -0.167\n",
      "0.82 -0.172\n",
      "0.83 -0.177\n",
      "0.84 -0.182\n",
      "0.85 -0.187\n",
      "0.86 -0.192\n",
      "0.87 -0.197\n",
      "0.88 -0.202\n",
      "0.89 -0.208\n",
      "0.9 -0.213\n",
      "0.91 -0.218\n",
      "0.92 -0.224\n",
      "0.93 -0.229\n",
      "0.94 -0.235\n",
      "0.95 -0.241\n",
      "0.96 -0.246\n",
      "0.97 -0.252\n",
      "0.98 -0.258\n",
      "0.99 -0.264\n",
      "1.0 -0.27\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "theta = 0.55\n",
    "r_f = 0.9\n",
    "p = 0.3\n",
    "delta = 0.05\n",
    "w0 = 0.8\n",
    "c_r = 0.3\n",
    "r = 0.6\n",
    "T = 26\n",
    "t0 = 0\n",
    "wT = 0\n",
    "\n",
    "rho = 1 / (1 + delta)\n",
    "gamma = rho\n",
    "\n",
    "best_f = 0\n",
    "w = w0\n",
    "epsilon=1e-4\n",
    "\n",
    "\n",
    "# Define the reward function (reward depends on state and action)\n",
    "def reward_function(state, action):\n",
    "    return (action * state * theta * r_f - p * action**2 + r * theta - c_r)\n",
    "\n",
    "for w in np.linspace(0, w0, 101):\n",
    "    for f in np.linspace(0, 1, 101):\n",
    "        print(round(f,2), round(reward_function(wT, f), 3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "c4019e29",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 0.512 -0.26999999999999996 -0.5271428571428571\n",
      "0 0.512 -0.26999999999999996 -0.5231828571428571\n",
      "0 0.512 -0.26999999999999996 -0.5192228571428571\n",
      "0 0.512 -0.26999999999999996 -0.515262857142857\n",
      "0 0.512 -0.26999999999999996 -0.5113028571428571\n",
      "0 0.512 -0.26999999999999996 -0.5073428571428571\n",
      "0 0.512 -0.26999999999999996 -0.5033828571428571\n",
      "0 0.512 -0.26999999999999996 -0.49942285714285706\n",
      "0 0.512 -0.26999999999999996 -0.4954628571428571\n",
      "0 0.512 -0.26999999999999996 -0.4915028571428571\n",
      "0 0.512 -0.26999999999999996 -0.48754285714285706\n",
      "0 0.512 -0.26999999999999996 -0.4835828571428571\n",
      "0 0.512 -0.26999999999999996 -0.4796228571428571\n",
      "0 0.512 -0.26999999999999996 -0.47566285714285705\n",
      "0 0.512 -0.26999999999999996 -0.4717028571428571\n",
      "0 0.512 -0.26999999999999996 -0.46774285714285707\n",
      "0 0.512 -0.26999999999999996 -0.46378285714285705\n",
      "0 0.512 -0.26999999999999996 -0.4598228571428571\n",
      "0 0.512 -0.26999999999999996 -0.45586285714285707\n",
      "0 0.512 -0.26999999999999996 -0.45190285714285705\n",
      "0 0.512 -0.26999999999999996 -0.4479428571428571\n",
      "0 0.512 -0.26999999999999996 -0.44398285714285707\n",
      "0 0.512 -0.26999999999999996 -0.44002285714285705\n",
      "0 0.512 -0.26999999999999996 -0.4360628571428571\n",
      "0 0.512 -0.26999999999999996 -0.43210285714285707\n",
      "0 0.512 -0.26999999999999996 -0.42814285714285705\n",
      "0 0.512 -0.26999999999999996 -0.4241828571428571\n",
      "0 0.512 -0.26999999999999996 -0.42022285714285706\n",
      "0 0.512 -0.26999999999999996 -0.41626285714285705\n",
      "0 0.512 -0.26999999999999996 -0.4123028571428571\n",
      "0 0.512 -0.26999999999999996 -0.40834285714285706\n",
      "0 0.512 -0.26999999999999996 -0.40438285714285704\n",
      "0 0.512 -0.26999999999999996 -0.4004228571428571\n",
      "0 0.512 -0.26999999999999996 -0.39646285714285706\n",
      "0 0.512 -0.26999999999999996 -0.39250285714285704\n",
      "0 0.512 -0.26999999999999996 -0.3885428571428571\n",
      "0 0.512 -0.26999999999999996 -0.384582857142857\n",
      "0 0.512 -0.26999999999999996 -0.38062285714285704\n",
      "0 0.512 -0.26999999999999996 -0.3766628571428571\n",
      "0 0.512 -0.26999999999999996 -0.3727028571428571\n",
      "0 0.512 -0.26999999999999996 -0.36874285714285704\n",
      "0 0.512 -0.26999999999999996 -0.3647828571428571\n",
      "0 0.512 -0.26999999999999996 -0.36082285714285706\n",
      "0 0.512 -0.26999999999999996 -0.35686285714285704\n",
      "0 0.512 -0.26999999999999996 -0.3529028571428571\n",
      "0 0.512 -0.26999999999999996 -0.34894285714285705\n",
      "0 0.512 -0.26999999999999996 -0.34498285714285704\n",
      "0 0.512 -0.26999999999999996 -0.34102285714285707\n",
      "0 0.512 -0.26999999999999996 -0.33706285714285705\n",
      "0 0.512 -0.26999999999999996 -0.33310285714285703\n",
      "0 0.512 -0.26999999999999996 -0.32914285714285707\n",
      "0 0.512 -0.26999999999999996 -0.325182857142857\n",
      "0 0.512 -0.26999999999999996 -0.32122285714285703\n",
      "0 0.512 -0.26999999999999996 -0.31726285714285707\n",
      "0 0.512 -0.26999999999999996 -0.3133028571428571\n",
      "0 0.512 -0.26999999999999996 -0.30934285714285703\n",
      "0 0.512 -0.26999999999999996 -0.305382857142857\n",
      "0 0.512 -0.26999999999999996 -0.30142285714285705\n",
      "0 0.512 -0.26999999999999996 -0.29746285714285703\n",
      "0 0.512 -0.26999999999999996 -0.293502857142857\n",
      "0 0.512 -0.26999999999999996 -0.28954285714285705\n",
      "0 0.512 -0.26999999999999996 -0.2855828571428571\n",
      "0 0.512 -0.26999999999999996 -0.281622857142857\n",
      "0 0.512 -0.26999999999999996 -0.27766285714285704\n",
      "0 0.512 -0.26999999999999996 -0.2737028571428571\n",
      "0 0.512 -0.26999999999999996 -0.26974285714285706\n",
      "0 0.512 -0.26999999999999996 -0.26578285714285704\n",
      "0 0.512 -0.26999999999999996 -0.261822857142857\n",
      "0 0.512 -0.26999999999999996 -0.25786285714285706\n",
      "0 0.512 -0.26999999999999996 -0.25390285714285704\n",
      "0 0.512 -0.26999999999999996 -0.24994285714285702\n",
      "0 0.512 -0.26999999999999996 -0.245982857142857\n",
      "0 0.512 -0.26999999999999996 -0.24202285714285698\n",
      "0 0.512 -0.26999999999999996 -0.23806285714285708\n",
      "0 0.512 -0.26999999999999996 -0.23410285714285706\n",
      "0 0.512 -0.26999999999999996 -0.23014285714285704\n",
      "0 0.512 -0.26999999999999996 -0.22618285714285702\n",
      "0 0.512 -0.26999999999999996 -0.22222285714285706\n",
      "0 0.512 -0.26999999999999996 -0.2182628571428571\n",
      "0 0.512 -0.26999999999999996 -0.21430285714285707\n",
      "0 0.512 -0.26999999999999996 -0.21034285714285705\n",
      "0 0.512 -0.26999999999999996 -0.20638285714285703\n",
      "0 0.512 -0.26999999999999996 -0.20242285714285702\n",
      "0 0.512 -0.26999999999999996 -0.19846285714285705\n",
      "0 0.512 -0.26999999999999996 -0.19450285714285703\n",
      "0 0.512 -0.26999999999999996 -0.19054285714285701\n",
      "0 0.512 -0.26999999999999996 -0.186582857142857\n",
      "0 0.512 -0.26999999999999996 -0.18262285714285698\n",
      "0 0.512 -0.26999999999999996 -0.17866285714285707\n",
      "0 0.512 -0.26999999999999996 -0.17470285714285705\n",
      "0 0.512 -0.26999999999999996 -0.17074285714285703\n",
      "0 0.512 -0.26999999999999996 -0.16678285714285707\n",
      "0 0.512 -0.26999999999999996 -0.16282285714285705\n",
      "0 0.512 -0.26999999999999996 -0.15886285714285708\n",
      "0 0.512 -0.26999999999999996 -0.15490285714285706\n",
      "0 0.512 -0.26999999999999996 -0.15094285714285705\n",
      "0 0.512 -0.26999999999999996 -0.14698285714285703\n",
      "0 0.512 -0.26999999999999996 -0.143022857142857\n",
      "0 0.512 -0.26999999999999996 -0.13906285714285704\n",
      "0 0.512 -0.26999999999999996 -0.13510285714285702\n",
      "0 0.512 -0.26999999999999996 -0.131142857142857\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "theta = 0.55\n",
    "r_f = 0.9\n",
    "p = 0.3\n",
    "delta = 0.05\n",
    "w0 = 0.8\n",
    "c_r = 0.3\n",
    "r = 0.6\n",
    "T = 26\n",
    "t0 = 0\n",
    "wT = 0\n",
    "\n",
    "rho = 1 / (1 + delta)\n",
    "gamma = rho\n",
    "\n",
    "best_f = 0\n",
    "w = w0\n",
    "epsilon=1e-4\n",
    "\n",
    "\n",
    "# Define the reward function (reward depends on state and action)\n",
    "def reward_function(state, action):\n",
    "    return (action * state * theta * r_f - p * action**2 + r * theta - c_r)\n",
    "\n",
    "for w in np.linspace(0, w0, 101):\n",
    "    for f in np.linspace(0, 1, 101):\n",
    "        obj_end = reward_function(wT, f)\n",
    "        cal_obj = reward_function(w, f) + gamma * reward_function(wT, f)\n",
    "        if obj_end > 0 and cal_obj > 0:\n",
    "            if (cal_obj - obj_end) < epsilon :\n",
    "                obj = round(cal_obj, 3)\n",
    "                best_f = round(f, 2)\n",
    "                cur_w = w\n",
    "                        \n",
    "    print(best_f, cur_w, obj_end, cal_obj)\n",
    "    \n",
    "t -= 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "893fa415",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3aa07769",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "theta = 0.55\n",
    "r_f = 0.9\n",
    "p = 0.3\n",
    "delta = 0.05\n",
    "w0 = 0.8\n",
    "c_r = 0.3\n",
    "r = 0.6\n",
    "T = 26\n",
    "t0 = 0\n",
    "\n",
    "rho = 1 / (1 + delta)\n",
    "gamma = rho\n",
    "\n",
    "best_f = 0\n",
    "w = w0\n",
    "\n",
    "# Define the transition dynamics function (for simplicity, a deterministic environment)\n",
    "def transition_function(state, action):\n",
    "        next_w = (1 - action) * state\n",
    "        next_w = np.clip(next_w, 0, 1)  # Clip to stay within [0, 1]\n",
    "        return next_w\n",
    "\n",
    "# Define the reward function (reward depends on state and action)\n",
    "def reward_function(state, action):\n",
    "    return (action * state * theta * r_f - p * action**2 + r * theta - c_r)\n",
    "\n",
    "for t in np.linspace (T, t0, T-t0+1):\n",
    "    obj = 0\n",
    "    obj_end = - p * action**2 + r * theta - c_r\n",
    "    wT = 0\n",
    "    \n",
    "    w_next = transition_function(w, best_f)\n",
    "    for f in np.linspace(0, 1, 101):\n",
    "        cal_obj = reward_function(w, f) + gamma * (reward_function(w_next, f) * (T - t))\n",
    "        #cal_obj = ((-f * w_next * theta * r_f) + (p * (f**2)) - (r * theta) + c_r) * (rho**t)\n",
    "        if cal_obj > obj:\n",
    "            obj = round(cal_obj, 3)\n",
    "            best_f = round(f, 2)\n",
    "            w = round(w_next, 2)\n",
    "    \n",
    "    print(t, best_f, w, obj)\n",
    "    \n",
    "    t -= 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7034118a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "theta = 0.55\n",
    "r_f = 0.9\n",
    "p = 0.3\n",
    "delta = 0.05\n",
    "w0 = 0.8\n",
    "c_r = 0.3\n",
    "r = 0.6\n",
    "T = 25\n",
    "t0 = 0\n",
    "\n",
    "rho = 1 / (1 + delta)\n",
    "gamma = rho\n",
    "\n",
    "best_f = 0\n",
    "w = w0\n",
    "\n",
    "V = np.zeros[num_states]\n",
    "\n",
    "# Define the transition dynamics function (for simplicity, a deterministic environment)\n",
    "def transition_function(state, action):\n",
    "        next_w = (1 - action) * state\n",
    "        next_w = np.clip(next_w, 0, 1)  # Clip to stay within [0, 1]\n",
    "        return next_w\n",
    "\n",
    "# Define the reward function (reward depends on state and action)\n",
    "def reward_function(state, action):\n",
    "    return (action * state * theta * r_f - p * action**2 + r * theta - c_r)\n",
    "\n",
    "for t in np.linspace(T, t0, T-t0+1):\n",
    "    obj = 0\n",
    "    #w_next = transition_function(w, best_f)\n",
    "    obj_end = - p * action**2 + r * theta - c_r\n",
    "    \n",
    "    for s in range(num_states):\n",
    "        \n",
    "        for f in np.linspace(0, 1, 101):\n",
    "            V[s] = \n",
    "        cal_obj = reward_function(w, f) + gamma * (reward_function(w_next, f) * (T - t))\n",
    "        #cal_obj = ((-f * w_next * theta * r_f) + (p * (f**2)) - (r * theta) + c_r) * (rho**t)\n",
    "        if cal_obj > obj:\n",
    "            obj = round(cal_obj, 3)\n",
    "            best_f = round(f, 2)\n",
    "            w = round(w_next, 2)\n",
    "    \n",
    "    print(t, best_f, w, obj)\n",
    "    \n",
    "    \n",
    "    \n",
    "    t -= 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "7dc952c2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "25.0\n",
      "24.0\n",
      "23.0\n",
      "22.0\n",
      "21.0\n",
      "20.0\n",
      "19.0\n",
      "18.0\n",
      "17.0\n",
      "16.0\n",
      "15.0\n",
      "14.0\n",
      "13.0\n",
      "12.0\n",
      "11.0\n",
      "10.0\n",
      "9.0\n",
      "8.0\n",
      "7.0\n",
      "6.0\n",
      "5.0\n",
      "4.0\n",
      "3.0\n",
      "2.0\n",
      "1.0\n",
      "0.0\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "theta = 0.55\n",
    "r_f = 0.9\n",
    "p = 0.3\n",
    "delta = 0.05\n",
    "w0 = 0.8\n",
    "c_r = 0.3\n",
    "r = 0.6\n",
    "T = 25\n",
    "t0 = 0\n",
    "\n",
    "rho = 1 / (1 + delta)\n",
    "gamma = rho\n",
    "\n",
    "best_f = 0\n",
    "w = w0\n",
    "\n",
    "V = np.zeros[num_states]\n",
    "\n",
    "# Define the transition dynamics function (for simplicity, a deterministic environment)\n",
    "def transition_function(state, action):\n",
    "        next_w = (1 - action) * state\n",
    "        next_w = np.clip(next_w, 0, 1)  # Clip to stay within [0, 1]\n",
    "        return next_w\n",
    "\n",
    "# Define the reward function (reward depends on state and action)\n",
    "def reward_function(state, action):\n",
    "    return (action * state * theta * r_f - p * action**2 + r * theta - c_r)\n",
    "\n",
    "for t in np.linspace(T, t0, T-t0+1):\n",
    "    obj = 0\n",
    "    w_next = transition_function(w, best_f)\n",
    "    for s in range(num_states):\n",
    "        \n",
    "    for f in np.linspace(0, 1, 101):\n",
    "        cal_obj = reward_function(w, f) + gamma * (reward_function(w_next, f) * (T - t))\n",
    "        #cal_obj = ((-f * w_next * theta * r_f) + (p * (f**2)) - (r * theta) + c_r) * (rho**t)\n",
    "        if cal_obj > obj:\n",
    "            obj = round(cal_obj, 3)\n",
    "            best_f = round(f, 2)\n",
    "            w = round(w_next, 2)\n",
    "    \n",
    "    print(t, best_f, w, obj)\n",
    "    \n",
    "    \n",
    "    \n",
    "    t -= 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9295a7cb",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df01a061",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
